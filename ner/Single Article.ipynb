{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "import nltk.data, pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import scrapy, json, ast, copy\n",
    "from klein import Klein, route, run\n",
    "from scrapy import signals\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "from scrapy.utils.log import configure_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "    Author: Chan Pruksapha\n",
    "    Base code : https://gist.github.com/onyxfish/322906\n",
    "    Changes :\n",
    "        1) Use Treebank Chunk (WSJ corpus based) in place of default chunk (ACE corpus)\n",
    "        2) Node maching from label 'NE' change to more complex rules\n",
    "'''\n",
    "\n",
    "ne_chunk_sents = pickle.load(open('treebank_chunk_ub.pickle'))\n",
    "\n",
    "def preprocess(text):\n",
    "    '''for i in range(len(tokenList)):\n",
    "        if tokenList[i] == u'–' :\n",
    "            tokenList[i] = u'-'\n",
    "        elif tokenList[i] == u'&':\n",
    "            tokenList[i] = u'Amps'''            \n",
    "    text = text.replace(u'–', u'-').replace(u'&', u'Amps')\n",
    "    return text\n",
    "\n",
    "def postprocess(entity_name):\n",
    "    return entity_name.replace('Amps ', '& ')\n",
    "\n",
    "def exclude(wordList):\n",
    "    \n",
    "    float_match = re.compile(r'[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?$').match   \n",
    "    def is_number_re(val):\n",
    "        return bool(float_match(val))\n",
    "\n",
    "    if len(wordList) == 1 and is_number_re(wordList[0]) :        \n",
    "        return True        \n",
    "    \n",
    "    return False\n",
    "    \n",
    "def rules(wordList, labelList):\n",
    "    \n",
    "    if exclude(wordList) :\n",
    "        return False\n",
    "\n",
    "    pairs = zip(wordList, labelList)\n",
    "    cleanedPairs = filter(lambda (w,l) : l != 'DT' and l!= 'IN', pairs)\n",
    "    if len(cleanedPairs) == 0 :\n",
    "        return False\n",
    "    \n",
    "    Ws, Ls = zip(*cleanedPairs)\n",
    "    \n",
    "    return all(map(lambda w:  w[0].upper() == w[0], Ws)) and Ls[0] == 'NNP'\n",
    "    \n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NP':\n",
    "            wordList = [child[0] for child in t]\n",
    "            labelList = [child[1] for child in t]\n",
    "            if rules(wordList, labelList):\n",
    "                entity_names.append(postprocess(' '.join([child[0] for child in t])))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    return entity_names\n",
    "\n",
    "def ne_extract(article):    \n",
    "    sentences = sent_tokenize(preprocess(article))\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = ne_chunk_sents.parse_sents(tagged_sentences)\n",
    "\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "    entity_set = list(set(entity_names))\n",
    "\n",
    "    return entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, urllib\n",
    "\n",
    "candidates = [\"Standard Life Investments\", \"Laith Khalaf\", \"Vyas\", \"Aviva Investors\", \"Keenan Vyas\", \"Thursday\", \"De Montfort University\", \"MAmpsG Investments\", \"Duff & Phelps\", \"Deka\", \"Canada Life\", \"the PRA ( Prudential Regulation Authority )\", \"Bernstein\", \"BlackRock Inc\", \"Monday\", \"Lloyds Banking Group\", \"Columbia Threadneedle\", \"Friday\", \"Director\", \"Germany\", \"Henderson Global Investors\", \"Bank\", \"Britain 's Financial Ombudsman Service\", \"Union Investment\", \"Lansdown\", \"Aberdeen Asset Management\", \"Wednesday\", \"the Real Estate Advisory Group\", \"Henderson Group\", \"RBS\", \"the Financial Conduct Authority 's Chief Executive\", \"England Governor Mark Carney\", \"Tuesday\", \"the Ameriprise Group\", \"Barclays\", \"London\", \"Mediobanca Securities\"];\n",
    "\n",
    "def prepareData(candidates) :\n",
    "    \n",
    "    def cleanEnt(e):\n",
    "        return re.sub(u'[\\u201d\\u201c\\u2019]','', e)\n",
    "    \n",
    "    candidates = filter(lambda e: len(e) > 0, map(cleanEnt,candidates))\n",
    "    url_list = map(lambda e: 'http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query={}'\n",
    "                   .format(urllib.quote_plus(e.encode('utf8'))), \n",
    "                   candidates)\n",
    "    \n",
    "    return (candidates, url_list)\n",
    "\n",
    "\n",
    "def nameMatching(symbol, entity) :\n",
    "\n",
    "    suffix = set(['plc','holdings', 'group','limited','company',\n",
    "                                   'entertainments', 'inc', 'corporation', 'corp', 'international', 'ltd'])\n",
    "    \n",
    "    day_names = set(map(lambda s:s.lower(),[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]));\n",
    "    month_names = set(map(lambda s:s.lower(),[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]));\n",
    "    city_names = set(map(lambda s: s.lower(),[\"London\", \"Manchester\", \"Bath\", \"Liverpool\", \"Bristol\", \"Chester\", \"Cambridge\", \"Oxford\"]));\n",
    "    if not symbol:\n",
    "        return False\n",
    "    else :\n",
    "        symbol = re.sub(ur\"[\\.,]+\", \"\", symbol) # Remove punctuation\n",
    "        entity = re.sub(ur\"[\\.,]+\", \"\", entity)\n",
    "        symbol_tokens = map(lambda s:s.lower(), symbol.split())\n",
    "        entity_tokens = map(lambda s:s.lower(), entity.split())\n",
    "        \n",
    "        if len(entity_tokens) == 1 and entity_tokens[0] in day_names.union(month_names).union(city_names):\n",
    "            #print entity_tokens[0]\n",
    "            return False\n",
    "                 \n",
    "        for i in range(len(entity_tokens)) :\n",
    "            if entity_tokens[i] != symbol_tokens[i] :\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class YahooSpider(scrapy.Spider):\n",
    "    name = \"cache\"\n",
    "    allowed_domains = [\"d.yimg.com\"]\n",
    "\n",
    "    def __init__(self, candidates=[], *args, **kwargs):\n",
    "        super(YahooSpider, self).__init__(*args, **kwargs)\n",
    "        self.entity_list, self.start_urls = prepareData(candidates)\n",
    "        self.entity_by_url = dict(zip(self.start_urls, self.entity_list))\n",
    "        self.company_entity_pair = []\n",
    "                    \n",
    "    def collect(self, json_response, url):\n",
    "        entity = self.entity_by_url[url]\n",
    "        yahoo_entity_pair = ((lambda a: a[0] if len(a) > 0 else None)(\n",
    "                 map(lambda d: (d[u'symbol'],d[u'name']), \n",
    "                             filter( lambda e: e[u'exchDisp'] in set(['London','NASDAQ','NYSE']) and e['typeDisp']=='Equity', \n",
    "                             json_response['ResultSet']['Result'])[:1])),\n",
    "                 entity)\n",
    "           \n",
    "        self.company_entity_pair.append(yahoo_entity_pair)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        json_response = json.loads(response.body_as_unicode())\n",
    "        self.collect(json_response, response.request.url)\n",
    "                   \n",
    "\n",
    "output_array = []\n",
    "\n",
    "def callback(spider, reason):\n",
    "    filteredRes = [c for (c,e) in filter(lambda (c, e): c and nameMatching(c[1], e), spider.company_entity_pair)]\n",
    "    #print filteredRes\n",
    "    output_array.extend(filteredRes);\n",
    "    #reactor.stop()\n",
    "\n",
    "#from twisted.internet import reactor\n",
    "#from scrapy.crawler import CrawlerRunner\n",
    "#from scrapy.utils.log import configure_logging\n",
    "\n",
    "#configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "#runner = CrawlerRunner()\n",
    "#YahooCralwer = runner.create_crawler(YahooSpider)\n",
    "\n",
    "#YahooCralwer.signals.connect(callback, signal=signals.spider_closed)    \n",
    "#d = runner.crawl(YahooCralwer, candidates)\n",
    "    \n",
    "#reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-31 10:45:04+0000 [-] Log opened.\n",
      "2016-10-31 10:45:04+0000 [-] Site starting on 8080\n",
      "2016-10-31 10:45:04+0000 [-] Starting factory <twisted.web.server.Site instance at 0x7f6b4d52ecf8>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Input: Search Query\n",
    "Output: List of Articles\n",
    "'''\n",
    "\n",
    "configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "runner = CrawlerRunner()\n",
    "app = Klein()\n",
    " \n",
    "     \n",
    "@app.route('/ner', methods=['POST'])\n",
    "def getSymbolList(request):\n",
    "    del output_array[:]\n",
    "    request.setHeader('Access-Control-Allow-Origin', '*')\n",
    "    post_msg = ast.literal_eval(request.content.read());\n",
    "    \n",
    "    candidates = ne_extract(post_msg['content'].decode('utf-8'))\n",
    "    \n",
    "    runner = CrawlerRunner()\n",
    "    YahooAllCralwer = runner.create_crawler(YahooSpider)\n",
    "    YahooAllCralwer.signals.connect(callback, signal=signals.spider_closed)    \n",
    "    d = runner.crawl(YahooAllCralwer, candidates)\n",
    "    d.addCallback(lambda res: json.dumps(\n",
    "            [{'ticker':t[0],'name':t[1]} for t in output_array]\n",
    "        ))\n",
    "    \n",
    "    return d\n",
    "\n",
    "app.run(host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f32f6757c8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'\\n\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnameMatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def nameMatching(symbol, entity) :\n",
    "\n",
    "    suffix = set(['plc','holdings', 'group','limited','company',\n",
    "                                   'entertainments', 'inc', 'corporation', 'corp', 'international', 'ltd'])\n",
    "    \n",
    "    day_names = set(map(lambda s:s.lower(),[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]));\n",
    "    month_names = set(map(lambda s:s.lower(),[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]));\n",
    "    city_names = set(map(lambda s: s.lower(),[\"London\", \"Manchester\", \"Bath\", \"Liverpool\", \"Bristol\", \"Chester\", \"Cambridge\", \"Oxford\"]));\n",
    "    if not symbol:\n",
    "        return False\n",
    "    else :\n",
    "        symbol = re.sub(ur\"[\\.,]+\", \"\", symbol) # Remove punctuation\n",
    "        entity = re.sub(ur\"[\\.,]+\", \"\", entity)\n",
    "        symbol_tokens = map(lambda s:s.lower(), symbol.split())\n",
    "        entity_tokens = map(lambda s:s.lower(), entity.split())\n",
    "        \n",
    "        if len(entity_tokens) == 1 and entity_tokens[0] in day_names.union(month_names).union(city_names):\n",
    "            return False\n",
    "                 \n",
    "        for i in range(len(entity_tokens)) :\n",
    "            if entity_tokens[i] != symbol_tokens[i] :\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "print '\\n\\n\\n'\n",
    "\n",
    "print json.dumps([dict(ticker=c[0],name=c[1]) for (c,e) in filter(lambda (c, e): c and nameMatching(c[1], e), temp)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import scrapy, json, ast, copy\n",
    "from klein import Klein, route, run\n",
    "from scrapy import signals\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "from scrapy.utils.log import configure_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "    Author: Chan Pruksapha\n",
    "    Modified from : https://gist.github.com/onyxfish/322906\n",
    "    Changes :\n",
    "        1) Use Treebank Chunk (WSJ corpus based) in place of default chunk (ACE corpus)\n",
    "        2) Node maching from label 'NE' change to more complex rules\n",
    "'''\n",
    "\n",
    "ne_chunk_sents = nltk.data.load('chunkers/treebank_chunk_ub.pickle')\n",
    "\n",
    "def preprocess(text):\n",
    "    '''for i in range(len(tokenList)):\n",
    "        if tokenList[i] == u'–' :\n",
    "            tokenList[i] = u'-'\n",
    "        elif tokenList[i] == u'&':\n",
    "            tokenList[i] = u'Amps'''            \n",
    "    text = text.replace(u'–', u'-').replace(u'&', u'Amps')\n",
    "    return text\n",
    "\n",
    "def postprocess(entity_name):\n",
    "    return entity_name.replace('Amps ', '& ')\n",
    "\n",
    "def exclude(wordList):\n",
    "    \n",
    "    float_match = re.compile(r'[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?$').match   \n",
    "    def is_number_re(val):\n",
    "        return bool(float_match(val))\n",
    "\n",
    "    if len(wordList) == 1 and is_number_re(wordList[0]) :        \n",
    "        return True        \n",
    "    \n",
    "    return False\n",
    "    \n",
    "def rules(wordList, labelList):\n",
    "    \n",
    "    if exclude(wordList) :\n",
    "        return False\n",
    "\n",
    "    pairs = zip(wordList, labelList)\n",
    "    cleanedPairs = filter(lambda (w,l) : l != 'DT' and l!= 'IN', pairs)\n",
    "    if len(cleanedPairs) == 0 :\n",
    "        return False\n",
    "    \n",
    "    Ws, Ls = zip(*cleanedPairs)\n",
    "    \n",
    "    return all(map(lambda w:  w[0].upper() == w[0], Ws)) and Ls[0] == 'NNP'\n",
    "    \n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NP':\n",
    "            wordList = [child[0] for child in t]\n",
    "            labelList = [child[1] for child in t]\n",
    "            if rules(wordList, labelList):\n",
    "                entity_names.append(postprocess(' '.join([child[0] for child in t])))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    return entity_names\n",
    "\n",
    "def ne_extract(article):    \n",
    "    sentences = sent_tokenize(preprocess(article))\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = ne_chunk_sents.parse_sents(tagged_sentences)\n",
    "\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "    entity_set = list(set(entity_names))\n",
    "\n",
    "    return entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, urllib\n",
    "\n",
    "candidates = [\"Standard Life Investments\", \"Laith Khalaf\", \"Vyas\", \"Aviva Investors\", \"Keenan Vyas\", \"Thursday\", \"De Montfort University\", \"MAmpsG Investments\", \"Duff & Phelps\", \"Deka\", \"Canada Life\", \"the PRA ( Prudential Regulation Authority )\", \"Bernstein\", \"BlackRock Inc\", \"Monday\", \"Lloyds Banking Group\", \"Columbia Threadneedle\", \"Friday\", \"Director\", \"Germany\", \"Henderson Global Investors\", \"Bank\", \"Britain 's Financial Ombudsman Service\", \"Union Investment\", \"Lansdown\", \"Aberdeen Asset Management\", \"Wednesday\", \"the Real Estate Advisory Group\", \"Henderson Group\", \"RBS\", \"the Financial Conduct Authority 's Chief Executive\", \"England Governor Mark Carney\", \"Tuesday\", \"the Ameriprise Group\", \"Barclays\", \"London\", \"Mediobanca Securities\"];\n",
    "\n",
    "def prepareData(candidates) :\n",
    "    \n",
    "    def cleanEnt(e):\n",
    "        return re.sub(u'[\\u201d\\u201c\\u2019]','', e)\n",
    "    \n",
    "    candidates = filter(lambda e: len(e) > 0, map(cleanEnt,candidates))\n",
    "    url_list = map(lambda e: 'http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query={}'\n",
    "                   .format(urllib.quote_plus(e.encode('utf8'))), \n",
    "                   candidates)\n",
    "    \n",
    "    return (candidates, url_list)\n",
    "\n",
    "\n",
    "def nameMatching(symbol, entity) :\n",
    "\n",
    "    suffix = set(['plc','holdings', 'group','limited','company',\n",
    "                                   'entertainments', 'inc', 'corporation', 'corp', 'international', 'ltd'])\n",
    "    \n",
    "    day_names = set(map(lambda s:s.lower(),[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]));\n",
    "    month_names = set(map(lambda s:s.lower(),[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]));\n",
    "    city_names = set(map(lambda s: s.lower(),[\"London\", \"Manchester\", \"Bath\", \"Liverpool\", \"Bristol\", \"Chester\", \"Cambridge\", \"Oxford\"]));\n",
    "    if not symbol:\n",
    "        return False\n",
    "    else :\n",
    "        symbol = re.sub(ur\"[\\.,]+\", \"\", symbol) # Remove punctuation\n",
    "        entity = re.sub(ur\"[\\.,]+\", \"\", entity)\n",
    "        symbol_tokens = map(lambda s:s.lower(), symbol.split())\n",
    "        entity_tokens = map(lambda s:s.lower(), entity.split())\n",
    "        \n",
    "        if len(entity_tokens) == 1 and entity_tokens[0] in day_names.union(month_names).union(city_names):\n",
    "            #print entity_tokens[0]\n",
    "            return False\n",
    "                 \n",
    "        for i in range(len(entity_tokens)) :\n",
    "            if entity_tokens[i] != symbol_tokens[i] :\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class YahooSpider(scrapy.Spider):\n",
    "    name = \"cache\"\n",
    "    allowed_domains = [\"d.yimg.com\"]\n",
    "\n",
    "    def __init__(self, candidates=[], *args, **kwargs):\n",
    "        super(YahooSpider, self).__init__(*args, **kwargs)\n",
    "        self.entity_list, self.start_urls = prepareData(candidates)\n",
    "        self.entity_by_url = dict(zip(self.start_urls, self.entity_list))\n",
    "        self.company_entity_pair = []\n",
    "                    \n",
    "    def collect(self, json_response, url):\n",
    "        entity = self.entity_by_url[url]\n",
    "        yahoo_entity_pair = ((lambda a: a[0] if len(a) > 0 else None)(\n",
    "                 map(lambda d: (d[u'symbol'],d[u'name']), \n",
    "                             filter( lambda e: e[u'exchDisp'] in set(['London','NASDAQ','NYSE']) and e['typeDisp']=='Equity', \n",
    "                             json_response['ResultSet']['Result'])[:1])),\n",
    "                 entity)\n",
    "           \n",
    "        self.company_entity_pair.append(yahoo_entity_pair)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        json_response = json.loads(response.body_as_unicode())\n",
    "        self.collect(json_response, response.request.url)\n",
    "                   \n",
    "\n",
    "output_array = []\n",
    "\n",
    "def callback(spider, reason):\n",
    "    filteredRes = [c for (c,e) in filter(lambda (c, e): c and nameMatching(c[1], e), spider.company_entity_pair)]\n",
    "    #print filteredRes\n",
    "    output_array.extend(filteredRes);\n",
    "    #reactor.stop()\n",
    "\n",
    "#from twisted.internet import reactor\n",
    "#from scrapy.crawler import CrawlerRunner\n",
    "#from scrapy.utils.log import configure_logging\n",
    "\n",
    "#configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "#runner = CrawlerRunner()\n",
    "#YahooCralwer = runner.create_crawler(YahooSpider)\n",
    "\n",
    "#YahooCralwer.signals.connect(callback, signal=signals.spider_closed)    \n",
    "#d = runner.crawl(YahooCralwer, candidates)\n",
    "    \n",
    "#reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:34:30+0700 [-] Log opened.\n",
      "2016-10-30 22:34:30+0700 [-] Site starting on 8080\n",
      "2016-10-30 22:34:30+0700 [-] Starting factory <twisted.web.server.Site instance at 0x114fb6050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO: Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:34:52+0700 [_GenericHTTPChannelProtocol,0,127.0.0.1] TelnetConsole starting on 6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Vyas> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Vyas> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Duff+%26+Phelps> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Duff+%26+Phelps> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Thursday> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Thursday> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Aviva+Investors> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Aviva+Investors> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=De+Montfort+University> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=De+Montfort+University> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=MAmpsG+Investments> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=MAmpsG+Investments> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Canada+Life> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Canada+Life> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Deka> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Deka> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+PRA+%28+Prudential+Regulation+Authority+%29> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+PRA+%28+Prudential+Regulation+Authority+%29> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Monday> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Monday> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=BlackRock+Inc> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=BlackRock+Inc> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Bernstein> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Bernstein> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Lloyds+Banking+Group> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Lloyds+Banking+Group> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Columbia+Threadneedle> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Columbia+Threadneedle> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Director> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Director> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Friday> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Friday> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Henderson+Global+Investors> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Henderson+Global+Investors> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Germany> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Germany> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Bank> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Bank> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Union+Investment> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Union+Investment> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Britain+%27s+Financial+Ombudsman+Service> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Britain+%27s+Financial+Ombudsman+Service> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Lansdown> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Lansdown> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Aberdeen+Asset+Management> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Aberdeen+Asset+Management> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Wednesday> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Wednesday> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Henderson+Group> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Henderson+Group> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Real+Estate+Advisory+Group> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Real+Estate+Advisory+Group> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=RBS> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=RBS> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Financial+Conduct+Authority+%27s+Chief+Executive> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Financial+Conduct+Authority+%27s+Chief+Executive> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=England+Governor+Mark+Carney> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=England+Governor+Mark+Carney> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Barclays> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Barclays> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Ameriprise+Group> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Ameriprise+Group> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Mediobanca+Securities> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Mediobanca+Securities> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=London> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=London> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Tuesday> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Tuesday> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Keenan+Vyas> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Keenan+Vyas> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Laith+Khalaf> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Laith+Khalaf> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Standard+Life+Investments> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Standard+Life+Investments> (referer: None)\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO: Closing spider (finished)\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 9628,\n",
      " 'downloader/request_count': 37,\n",
      " 'downloader/request_method_count/GET': 37,\n",
      " 'downloader/response_bytes': 27947,\n",
      " 'downloader/response_count': 37,\n",
      " 'downloader/response_status_count/200': 37,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 34, 59, 331718),\n",
      " 'log_count/DEBUG': 38,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 37,\n",
      " 'scheduler/dequeued': 37,\n",
      " 'scheduler/dequeued/memory': 37,\n",
      " 'scheduler/enqueued': 37,\n",
      " 'scheduler/enqueued/memory': 37,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 34, 52, 684336)}\n",
      "INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 9628,\n",
      " 'downloader/request_count': 37,\n",
      " 'downloader/request_method_count/GET': 37,\n",
      " 'downloader/response_bytes': 27947,\n",
      " 'downloader/response_count': 37,\n",
      " 'downloader/response_status_count/200': 37,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 34, 59, 331718),\n",
      " 'log_count/DEBUG': 38,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 37,\n",
      " 'scheduler/dequeued': 37,\n",
      " 'scheduler/dequeued/memory': 37,\n",
      " 'scheduler/enqueued': 37,\n",
      " 'scheduler/enqueued/memory': 37,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 34, 52, 684336)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n",
      "INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:34:59+0700 [-] \"127.0.0.1\" - - [30/Oct/2016:15:34:58 +0000] \"POST /ner HTTP/1.1\" 200 379 \"http://localhost:8000/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\"\n",
      "2016-10-30 22:34:59+0700 [-] (TCP Port 6023 Closed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO: Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:42:40+0700 [_GenericHTTPChannelProtocol,1,127.0.0.1] TelnetConsole starting on 6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO: Closing spider (finished)\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 42, 40, 773609),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 42, 40, 767531)}\n",
      "INFO: Dumping Scrapy stats:\n",
      "{'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 42, 40, 773609),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 42, 40, 767531)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n",
      "INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:42:40+0700 [-] \"127.0.0.1\" - - [30/Oct/2016:15:42:40 +0000] \"POST /ner HTTP/1.1\" 200 2 \"http://localhost:8000/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\"\n",
      "2016-10-30 22:42:40+0700 [-] (TCP Port 6023 Closed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO: Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:43:18+0700 [_GenericHTTPChannelProtocol,1,127.0.0.1] TelnetConsole starting on 6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Diageo> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Diageo> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Balfour+Beatty> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Balfour+Beatty> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Hammerson> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Hammerson> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=The+Share+Centre> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=The+Share+Centre> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Vodafone> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Vodafone> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=BP> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=BP> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=November+2007> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=November+2007> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Bloomsbury+Financial+Planning> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Bloomsbury+Financial+Planning> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Premier+Foods> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Premier+Foods> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Marks+%26+Spencer> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Marks+%26+Spencer> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Graham+Spooner> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Graham+Spooner> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Pearson> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Pearson> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+UK> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+UK> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Green+King> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Green+King> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=British+Land> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=British+Land> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=HSBC> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=HSBC> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Goldman+Sachs> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Goldman+Sachs> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=AstraZeneca> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=AstraZeneca> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Ladbrokes> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Ladbrokes> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Jason+Butler> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Jason+Butler> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Unilever> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Unilever> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=January+2007> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=January+2007> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+FTSE+250> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+FTSE+250> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Imperial+Tobacco> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Imperial+Tobacco> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Financial+Times> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+Financial+Times> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Royal+Dutch+Shell> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Royal+Dutch+Shell> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Carillion> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Carillion> (referer: None)\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO: Closing spider (finished)\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 6906,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 20928,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 43, 22, 635278),\n",
      " 'log_count/DEBUG': 28,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 43, 18, 13522)}\n",
      "INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 6906,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 20928,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 43, 22, 635278),\n",
      " 'log_count/DEBUG': 28,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 43, 18, 13522)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n",
      "INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:43:22+0700 [-] \"127.0.0.1\" - - [30/Oct/2016:15:43:22 +0000] \"POST /ner HTTP/1.1\" 200 675 \"http://localhost:8000/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\"\n",
      "2016-10-30 22:43:22+0700 [-] (TCP Port 6023 Closed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO: Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:43:33+0700 [_GenericHTTPChannelProtocol,1,127.0.0.1] TelnetConsole starting on 6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO: Closing spider (finished)\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 43, 33, 724879),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 43, 33, 718259)}\n",
      "INFO: Dumping Scrapy stats:\n",
      "{'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 43, 33, 724879),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 43, 33, 718259)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n",
      "INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:43:33+0700 [-] \"127.0.0.1\" - - [30/Oct/2016:15:43:33 +0000] \"POST /ner HTTP/1.1\" 200 2 \"http://localhost:8000/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\"\n",
      "2016-10-30 22:43:33+0700 [-] (TCP Port 6023 Closed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO: Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:45:30+0700 [_GenericHTTPChannelProtocol,1,127.0.0.1] TelnetConsole starting on 6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Sales> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Sales> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+UK+Read> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=the+UK+Read> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=J> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=J> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=December> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=December> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Sharp> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Sharp> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=November> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=November> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=October> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=October> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Christmas> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Christmas> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Michael+Sharp> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Michael+Sharp> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=August> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=August> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Advertisement+Debenhams> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Advertisement+Debenhams> (referer: None)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Debenhams> (referer: None)\n",
      "DEBUG: Crawled (200) <GET http://d.yimg.com/aq/autoc?lang=en-GB&region=UK&query=Debenhams> (referer: None)\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO: Closing spider (finished)\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 3033,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 9653,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 12,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 45, 32, 962362),\n",
      " 'log_count/DEBUG': 13,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 45, 30, 726951)}\n",
      "INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 3033,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 9653,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 12,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2016, 10, 30, 15, 45, 32, 962362),\n",
      " 'log_count/DEBUG': 13,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2016, 10, 30, 15, 45, 30, 726951)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n",
      "INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-30 22:45:32+0700 [-] \"127.0.0.1\" - - [30/Oct/2016:15:45:32 +0000] \"POST /ner HTTP/1.1\" 200 95 \"http://localhost:8000/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\"\n",
      "2016-10-30 22:45:32+0700 [-] (TCP Port 6023 Closed)\n",
      "2016-10-30 22:49:09+0700 [-] Received SIGINT, shutting down.\n",
      "2016-10-30 22:49:09+0700 [twisted.web.server.Site] (TCP Port 8080 Closed)\n",
      "2016-10-30 22:49:09+0700 [-] Stopping factory <twisted.web.server.Site instance at 0x114fb6050>\n",
      "2016-10-30 22:49:10+0700 [-] Main loop terminated.\n",
      "2016-10-30 22:49:10+0700 [-] \n",
      "2016-10-30 22:49:10+0700 [-] \n",
      "2016-10-30 22:49:10+0700 [-] \n",
      "2016-10-30 22:49:10+0700 [-] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Input: Search Query\n",
    "Output: List of Articles\n",
    "'''\n",
    "\n",
    "configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "runner = CrawlerRunner()\n",
    "app = Klein()\n",
    " \n",
    "     \n",
    "@app.route('/ner', methods=['POST'])\n",
    "def getSymbolList(request):\n",
    "    del output_array[:]\n",
    "    request.setHeader('Access-Control-Allow-Origin', '*')\n",
    "    post_msg = ast.literal_eval(request.content.read());\n",
    "    \n",
    "    candidates = ne_extract(post_msg['content'].decode('utf-8'))\n",
    "    \n",
    "    runner = CrawlerRunner()\n",
    "    YahooAllCralwer = runner.create_crawler(YahooSpider)\n",
    "    YahooAllCralwer.signals.connect(callback, signal=signals.spider_closed)    \n",
    "    d = runner.crawl(YahooAllCralwer, candidates)\n",
    "    d.addCallback(lambda res: json.dumps(\n",
    "            [{'ticker':t[0],'name':t[1]} for t in output_array]\n",
    "        ))\n",
    "    \n",
    "    return d\n",
    "\n",
    "app.run(host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f32f6757c8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'\\n\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnameMatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def nameMatching(symbol, entity) :\n",
    "\n",
    "    suffix = set(['plc','holdings', 'group','limited','company',\n",
    "                                   'entertainments', 'inc', 'corporation', 'corp', 'international', 'ltd'])\n",
    "    \n",
    "    day_names = set(map(lambda s:s.lower(),[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]));\n",
    "    month_names = set(map(lambda s:s.lower(),[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]));\n",
    "    city_names = set(map(lambda s: s.lower(),[\"London\", \"Manchester\", \"Bath\", \"Liverpool\", \"Bristol\", \"Chester\", \"Cambridge\", \"Oxford\"]));\n",
    "    if not symbol:\n",
    "        return False\n",
    "    else :\n",
    "        symbol = re.sub(ur\"[\\.,]+\", \"\", symbol) # Remove punctuation\n",
    "        entity = re.sub(ur\"[\\.,]+\", \"\", entity)\n",
    "        symbol_tokens = map(lambda s:s.lower(), symbol.split())\n",
    "        entity_tokens = map(lambda s:s.lower(), entity.split())\n",
    "        \n",
    "        if len(entity_tokens) == 1 and entity_tokens[0] in day_names.union(month_names).union(city_names):\n",
    "            return False\n",
    "                 \n",
    "        for i in range(len(entity_tokens)) :\n",
    "            if entity_tokens[i] != symbol_tokens[i] :\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "print '\\n\\n\\n'\n",
    "\n",
    "print json.dumps([dict(ticker=c[0],name=c[1]) for (c,e) in filter(lambda (c, e): c and nameMatching(c[1], e), temp)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
